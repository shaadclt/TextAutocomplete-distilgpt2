{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iR73u-aE4h8j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KMP_DUPLICATE_LIB_OK'] = \"TRUE\""
      ],
      "metadata": {
        "id": "4neCirH44xIU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"shibing624/code-autocomplete-distilgpt2-python\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"shibing624/code-autocomplete-distilgpt2-python\")"
      ],
      "metadata": {
        "id": "BvNRX-H-49Ts"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"The report should be given to\"]\n",
        "\n",
        "for prompt in prompt:\n",
        "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt')\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    outputs = model.generate(input_ids=input_ids,\n",
        "                             attention_mask = attention_mask,\n",
        "                             max_length=64 + len(prompt),\n",
        "                             temperature=1.0,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             repetition_penalty=1.0,\n",
        "                             do_sample=True,\n",
        "                             num_return_sequences=1,\n",
        "                             length_penalty=2.0,\n",
        "                             early_stopping=True)\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated_words = decoded.split()  # Split the generated text into words\n",
        "    if len(generated_words) > len(prompt.split()) + 1:  # Ensure there are at least two words generated after the prompt\n",
        "        print(prompt + ' ' + ' '.join(generated_words[len(prompt.split()):len(prompt.split())+2]))  # Print the first two words after the prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHjjQYE95dZ2",
        "outputId": "e7212860-06ea-4107-db81-2d1deb5a92af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The report should be given to the full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrT5VcJj79UH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}